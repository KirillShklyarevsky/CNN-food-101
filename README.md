# 1) Графики обучения для нейронной сети EfficientNet-B0 с использованием Transfer Learning и техники аугментации данных "Случайное горизонтальное и вертикальное отображение"
 Изменения типов отображения.
```
    data_augmentation = tf.keras.layers.experimental.preprocessing.RandomFlip(mode="horizontal", seed=None, name=None)(inputs)
```
```
    data_augmentation = tf.keras.layers.experimental.preprocessing.RandomFlip(mode="vertical", seed=None, name=None)(inputs)
```
```
    data_augmentation = tf.keras.layers.experimental.preprocessing.RandomFlip(mode="horizontal_and_vertical", seed=None, name=None)(inputs)
```
  - Легенда:

   ![](./Images/Flip_Acur.png)
  
   График метрики качества (на валидации):
   ![SVG example](./Images/epoch_categorical_accuracy_1.svg)

  - Легенда:

   ![](./Images/Flip_Loss.png)

  График функции потерь (на валидации):
   ![SVG example](./Images/epoch_loss_1.svg)

Наилучшим параметром отображения оказалось горизонтальное отображение. При нем значения метрики точности достигло 67,58%. На графике функции потерь горизонтального отображения наблюдаются наименьшие значения 1,208. 

Результат применения техники аугментации данных "Горизонтальное отображение":

![](./Images/img_horizontal.jpg)

# 2) Графики обучения для нейронной сети EfficientNet-B0 с использованием Transfer Learning и техники аугментации данных "Использование случайной части изображения" 
 
 Начальными размерами изображения были выбраны: 250x250, 275x275, 300x300, 250x300, 300x250.
 
  - Легенда:

   ![](./Images/Crop_Accur.png)
  
   График метрики качества (на валидации):
   ![SVG example](./Images/epoch_categorical_accuracy_2.svg)
   
   - Легенда:

   ![](./Images/Crop_Loss.png)

  График функции потерь (на валидации):
   ![SVG example](./Images/epoch_loss_2.svg)
   
   Наилучшим параметром изначаьным размером оказался 300x250. При котором значения метрики точности достигло 66,65%. На графике функции потерь наблюдаются наименьшие значения среди других начальных параметрах - 1,24. 
   
   Результат применения техники аугментации данных "Горизонтальное отображение":

![](./Images/img_crop.jpg)

# 3) Графики обучения для нейронной сети EfficientNet-B0 с использованием политики изменения темпа обучения - косинусное затухание с перезапусками.
 Изменения начальных темпов обучения для косинусного затухания с перезапусками.
```
    LearningRateScheduler(tf.keras.experimental.CosineDecayRestarts(0.001, 1000, 2.0, 1.0, 0.0, None))
```
```
    LearningRateScheduler(tf.keras.experimental.CosineDecayRestarts(0.0001, 1000, 2.0, 1.0, 0.0, None))
```
```
    LearningRateScheduler(tf.keras.experimental.CosineDecayRestarts(0.0002, 1000, 2.0, 1.0, 0.0, None))
```
  - Легенда:

   ![](./Images/Rot_Accur.png)
  
   График метрики качества (на валидации):
   ![SVG example](./Images/epoch_categorical_accuracy_3.svg)
   
   - Легенда:

   ![](./Images/Rot_Locc.png)

  График функции потерь (на валидации):
   ![SVG example](./Images/epoch_loss_3.svg)
   
   # 4) Графики обучения наилучших темпов обучения для фиксированных темпов обучения, косинусного затухания и косинусного затухания с перезапусками.
  - Легенда:

   ![](./Images/Com_Acc.png)
  
   График метрики качества (на валидации):
   ![SVG example](./Images/epoch_categorical_accuracy_4.svg)
   
   - Легенда:

   ![](./Images/Comb_Loss.png)

  График функции потерь (на валидации):
   ![SVG example](./Images/epoch_loss_4.svg)
   
# 5) Анализ полученных результатов

   Для каждого из методов выбора темпа обучения можно выбрать оптимальные параметры, а именно для фиксированного темпа обучения максимальная точность на валидации 67,48% при темпе обучения 0.0001. В случае с косинусным затуханием стал метод с начальным темпом обучения 0.0001 на котором точность на валидации составила 67,54%, а для косинусного затухания с перезапусками 67,61% так же при начальном темпе обучения 0.0001. Можно сделать вывод, что оптимальным значением темпа обучения яв-ся 0.0001 т.к на нем были достигнуты максимальная точность и минимальная ошибка на валидации. По сравнению методов, все методы при оптимальном значении темпа обучения показали практически одинаковый результат.
